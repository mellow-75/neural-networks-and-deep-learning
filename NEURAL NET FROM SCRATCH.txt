*********************NEURAL NET FROM SCRATCH*******************************
1) feed input data get output 

2)calculate error 

3)adjust parameters to minimise error

4) start again




data=np.array(df)    <----- converting dataframe to array




np.random.shuffle(data)
data_dev=data[0:1000].T                    <----- taking 1000 data points from the dataframe 
								   for validation 
y_dev=data_dev[0]                          <-------the first row is the labels as we 									   transposed the array 
x_dev=data_dev[1:n]


data_train=data[1000:m].T
y_train=data_train[0]
x_train=data_train[1:n]








initialising the random weights and biases for the net

def init_params():                              <---usasble by the whole model
    w1=np.random.rand(10,784)                   <---- rand creates an array containing 										random numbers the two numbers in the
    b1=np.random.rand(10,1)                           bracket give dimensions of the array.    
    
    w2=np.random.rand(10,10)
    b2=np.random.rand(10,1)




w1=np.random.rand(10,784)    these are the weights for the input to first dense layer ,
					from 784 inputs to 10 node in the dense layer 
					so one input to 10 nodes therefore in total 784 x 10 weights











******** defining the activation function ReLU



def ReLU(z):
    return np.maximum(0,z)                       <--relu activation 





********defining softmax activation 

def softmax(z):
    return math.exp(z)/np.sum(math.exp(z))



*******forward propogation of outputs from each layer


def forward_prop(w1,b1,w2,b2,x):           <----random wewights to begin with and the input 
                                                  from the data
    z1=w1.dot(x)+b1                        <----- first sumation from the input layer
    a1=ReLU(z1)                            <------output from the relu activation 
    z2=w2.dot(a1)+b2                        <------second sumation using the output from                  								relu activation , i.e 10 outputs form relu
    a2=softmax(z2)                          <-----second activation 
    return z1,a1,z2,a2                      a2 represents the final predicted output






*********getting one hot encoded y for calculating error


def one_hot(y):
    one_hot_y=np.zeros(y.size,y.max()+1)     <------ creating an array of 0 of 41000, 10
    one_hot_y[np.arange(y.size).y]=1
    return one_hot_y.T


    


*******************************arnage function*****************************

one_hot_y[np.arange(y.size).y]=1     

here we passed the dimension inside the bracket for the one_hot_y array, 
so what's happening is that , we entered two arrays of len 10 into the brackets which are then specifying which element is to be made equal to 1

so the program just zips the two arrays together and thus forms pairs like 

0,first value in the y array , suppose the label is 5

so the code will go to the 0th row and 5th coloumn and make it equal to 1
 as the labels and their positions are conicident therefore the label can be used as an inducation for the position of the label as well



------------------------------------------------------------------


def deriv_ReLU(z):
    return z>0 

got the derivative of the activation which will then be used in backpropogation 
to modify the values of parameters





def back_prop(z1,a1,z2,a2,w2,y):
    one_hot_y=one_hot(y)
    dz2=a2-one_hot_y
    dw2=1/m*dz2.dot(a1.T)
    db2=1/m*np.sum(dz2,2)
    dz1=w2.T.dot(dz2)*dervi_ReLU(z1)
    dw1=1/m*dz1.dot(x.T)
    
    db2=1/m*np.sum(dz1,2)
 


code implementation of concepts of ml learned in the course 
implementing adagrad , adam , mini batch gradient etc
implementing any reasearch paper
computer vision concepts from stanford 


























